{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from lbs.dl.encoders import enc_seq_onehot, enc_label\n",
    "import pickle\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open('./../3_Train_Test_Split/out/train_data.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    entries = []\n",
    "    for entry, seq, label in zip(df.index, df['sequence'].tolist(), df['socket_assignment'].tolist()):\n",
    "        if len(seq) <= 500:\n",
    "            pad_left = random.randint(0, 500 - len(seq))\n",
    "            sequences.append(enc_seq_onehot(seq, pad_length=500, pad_left=pad_left))\n",
    "            labels.append(enc_label(label, pad_length=500, pad_left=pad_left))\n",
    "            entries.append(entry)\n",
    "    return (np.asarray(sequences), np.asarray(labels), entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(pred, enc_seq):\n",
    "    decoded_preds = []\n",
    "    for entry_pred, entry in zip(pred, enc_seq):\n",
    "        decoded_pred = []\n",
    "        for pos_pred, pos_seq in zip(entry_pred, entry):\n",
    "            if not np.array_equal(pos_seq, np.zeros(20)):\n",
    "                decoded_pred.append(pos_pred[1])\n",
    "        decoded_preds.append(np.asarray(decoded_pred))\n",
    "    return np.asarray(decoded_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from lbs.dl.logger import Logger\n",
    "from lbs.dl.metrics import total_accuracy\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SpatialDropout1D, SpatialDropout2D, MaxPooling1D, MaxPooling2D, Dense, TimeDistributed, Convolution1D, BatchNormalization,Input,merge,LSTM, Dropout,Embedding,Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "def wcc(weights):\n",
    "    weights = K.variable(weights)\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    return loss\n",
    "weights = np.array([1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model():\n",
    "    lr = 0.0005\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(64, 28, padding='same', activation='relu', kernel_regularizer=l2(0.0001), input_shape=(500, 20)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution1D(64, 21, padding='same',  activation='relu', kernel_regularizer=l2(0.0001)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dense(2, activation='softmax', name='out'))\n",
    "    adam = Adam(lr=lr)\n",
    "    model.compile(optimizer=adam, loss=wcc(weights), metrics=[total_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV #1:\n",
      "Best F1 score: 0.294 (prec: 0.256, sens: 0.343)\n",
      "Best F1 score: 0.305 (prec: 0.353, sens: 0.268)\n",
      "Best F1 score: 0.334 (prec: 0.286, sens: 0.402)\n",
      "Best F1 score: 0.349 (prec: 0.366, sens: 0.334)\n",
      "Best F1 score: 0.389 (prec: 0.315, sens: 0.508)\n",
      "Best F1 score: 0.405 (prec: 0.369, sens: 0.448)\n",
      "2088\n",
      "CV #2:\n",
      "Best F1 score: 0.004 (prec: 0.696, sens: 0.002)\n",
      "Best F1 score: 0.305 (prec: 0.303, sens: 0.308)\n",
      "Best F1 score: 0.320 (prec: 0.324, sens: 0.317)\n",
      "Best F1 score: 0.344 (prec: 0.345, sens: 0.344)\n",
      "Best F1 score: 0.398 (prec: 0.348, sens: 0.464)\n",
      "Best F1 score: 0.402 (prec: 0.337, sens: 0.496)\n",
      "Best F1 score: 0.404 (prec: 0.379, sens: 0.433)\n",
      "4176\n",
      "CV #3:\n",
      "Best F1 score: 0.018 (prec: 0.746, sens: 0.009)\n",
      "Best F1 score: 0.196 (prec: 0.496, sens: 0.122)\n",
      "Best F1 score: 0.353 (prec: 0.315, sens: 0.402)\n",
      "Best F1 score: 0.364 (prec: 0.502, sens: 0.286)\n",
      "Best F1 score: 0.365 (prec: 0.267, sens: 0.576)\n",
      "Best F1 score: 0.398 (prec: 0.361, sens: 0.443)\n",
      "Best F1 score: 0.406 (prec: 0.386, sens: 0.427)\n",
      "Best F1 score: 0.407 (prec: 0.432, sens: 0.385)\n",
      "6264\n",
      "CV #4:\n",
      "Best F1 score: 0.093 (prec: 0.635, sens: 0.050)\n",
      "Best F1 score: 0.317 (prec: 0.248, sens: 0.441)\n",
      "Best F1 score: 0.356 (prec: 0.278, sens: 0.496)\n",
      "Best F1 score: 0.384 (prec: 0.289, sens: 0.572)\n",
      "Best F1 score: 0.408 (prec: 0.440, sens: 0.381)\n",
      "Best F1 score: 0.414 (prec: 0.421, sens: 0.408)\n",
      "Best F1 score: 0.426 (prec: 0.377, sens: 0.489)\n",
      "8351\n",
      "CV #5:\n",
      "Best F1 score: 0.028 (prec: 0.688, sens: 0.014)\n",
      "Best F1 score: 0.305 (prec: 0.431, sens: 0.236)\n",
      "Best F1 score: 0.356 (prec: 0.340, sens: 0.374)\n",
      "Best F1 score: 0.390 (prec: 0.344, sens: 0.449)\n",
      "Best F1 score: 0.417 (prec: 0.445, sens: 0.392)\n",
      "Best F1 score: 0.425 (prec: 0.375, sens: 0.492)\n",
      "10438\n"
     ]
    }
   ],
   "source": [
    "cv_results = {}\n",
    "for i in range(1, 6):\n",
    "    print('CV #%s:' % i)\n",
    "    ch1 = Logger(lab_pos=1, out_path='models/final_seq/', out_fn='final_seq_%s.h5' % (i), out_log='final_seq.txt' )\n",
    "    callbacks_list = [ch1]\n",
    "    model = Model()\n",
    "    ### Get data\n",
    "    val_df = df[df['val'] == i]\n",
    "    train_df = df[df['val'] != i]\n",
    "    train_data= encode(train_df)\n",
    "    valid_data = encode(val_df)\n",
    "    ### Train\n",
    "    h = model.fit(train_data[0], train_data[1],\n",
    "                  validation_data = (valid_data[0], valid_data[1]),\n",
    "                  batch_size=64,\n",
    "                  epochs=50,\n",
    "                  verbose=0, callbacks=callbacks_list)\n",
    "    ### Predict\n",
    "    model.load_weights('models/final_seq/final_seq_%s.h5' % i)\n",
    "    preds = model.predict(valid_data[0])\n",
    "    ### Decode predictions and add results to dict\n",
    "    decoded_preds = decode(preds, valid_data[0])\n",
    "    for entry, decoded_pred in zip(valid_data[2], decoded_preds):\n",
    "        assignment  = ''.join(str(int(label)) for label in np.rint(decoded_pred))\n",
    "        cv_results[entry] = assignment\n",
    "    print(len(cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10438, (10438, 25))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_results), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deepcoil = pd.DataFrame.from_dict(cv_results, orient='index')\n",
    "df_deepcoil.columns = ['deepcoil_assignment']\n",
    "df = pd.concat([df, df_deepcoil], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('out/seq/cv_results.p')\n",
    "df.to_csv('out/seq/cv_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pickle.load(open('./../3_Train_Test_Split/out/test_data.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = encode(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ensemble_results = {}\n",
    "for i in range(1, 6):\n",
    "    model.load_weights('models/final_seq/final_seq_%s.h5' % i)\n",
    "    preds = model.predict(test_data[0])\n",
    "    decoded_preds = decode(preds, test_data[0])\n",
    "    for decoded_pred, entry in zip(decoded_preds, test_data[2]):\n",
    "        if i == 1:\n",
    "            test_ensemble_results[entry] = decoded_pred\n",
    "        else:\n",
    "            test_ensemble_results[entry] = np.vstack((test_ensemble_results[entry], decoded_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_avg_results = {}\n",
    "for key, value in test_ensemble_results.items():\n",
    "    avg_results = np.average(value, axis=0)\n",
    "    assignment  = ''.join(str(int(label)) for label in np.rint(avg_results))\n",
    "    test_avg_results[key] = assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193, (1193, 24))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_avg_results), df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deepcoil = pd.DataFrame.from_dict(test_avg_results, orient='index')\n",
    "df_deepcoil.columns = ['deepcoil_assignment']\n",
    "df_test = pd.concat([df_test, df_deepcoil], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_pickle('out/seq/test_results.p')\n",
    "df_test.to_csv('out/seq/test_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Li (2016) benchmark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pickle.load(open('./../3_Train_Test_Split/out/li2016.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = encode(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ensemble_results = {}\n",
    "for i in range(1, 6):\n",
    "    model.load_weights('models/final_seq/final_seq_%s.h5' % i)\n",
    "    preds = model.predict(test_data[0])\n",
    "    decoded_preds = decode(preds, test_data[0])\n",
    "    for decoded_pred, entry in zip(decoded_preds, test_data[2]):\n",
    "        if i == 1:\n",
    "            test_ensemble_results[entry] = decoded_pred\n",
    "        else:\n",
    "            test_ensemble_results[entry] = np.vstack((test_ensemble_results[entry], decoded_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_avg_results = {}\n",
    "for key, value in test_ensemble_results.items():\n",
    "    avg_results = np.average(value, axis=0)\n",
    "    assignment  = ''.join(str(int(label)) for label in np.rint(avg_results))\n",
    "    test_avg_results[key] = assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(518, (518, 24))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_avg_results), df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deepcoil = pd.DataFrame.from_dict(test_avg_results, orient='index')\n",
    "df_deepcoil.columns = ['deepcoil_assignment']\n",
    "df_test = pd.concat([df_test, df_deepcoil], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_pickle('out/seq/li2016_results.p')\n",
    "df_test.to_csv('out/seq/li2016_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
